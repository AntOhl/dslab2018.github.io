{"paragraphs":[{"text":"%md\n### First steps with Hive\n\nCreate your database on hive. We will name it with your EPFL gaspar name.\n\n```shell\n%jdbc(hive)\ncreate database if not exists your_gaspar_name\n  location '/user/your_gaspar_name/hive'; \n```\n\nCreate an external Hive table. Hive will create a reference to the files but it will not manage the files itself. If you drop the table, only the definition in Hive is deleted. This exercise will work only if you have completed the HDFS exercises.\n\n```shell\n%jdbc(hive)\ncreate external table if not exists your_gaspar_name.traffic_count(Sdate string,Cosit int,Study int,Period int,LaneNumber int,LaneDescription string,LaneDirection int,DirectionDescription string,Volume int,Flags int,FlagText string,Setup int,NumBins int,Bins int)\n    row format delimited fields terminated by ','\n    stored as textfile\n    location '/user/your_gaspar_name/work1/';\n```\n\nNow verify that your table was properly created.\n```shell\n%jdbc(hive)\nselect * from your_gaspar_name.traffic_count limit 10;\n```\n\nReplace your_gaspar_name with your username and try the above commands in the next cells, create as many cells as needed.\n\nNote the first row. what did we do wrong?\n","user":"ebouille","dateUpdated":"2018-03-21T08:55:40+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>First steps with Hive</h3>\n<p>Create your database on hive. We will name it with your EPFL gaspar name.</p>\n<pre><code class=\"shell\">%jdbc(hive)\ncreate database if not exists your_gaspar_name\n  location '/user/your_gaspar_name/hive'; \n</code></pre>\n<p>Create an external Hive table. Hive will create a reference to the files but it will not manage the files itself. If you drop the table, only the definition in Hive is deleted. This exercise will work only if you have completed the HDFS exercises.</p>\n<pre><code class=\"shell\">%jdbc(hive)\ncreate external table if not exists your_gaspar_name.traffic_count(Sdate string,Cosit int,Study int,Period int,LaneNumber int,LaneDescription string,LaneDirection int,DirectionDescription string,Volume int,Flags int,FlagText string,Setup int,NumBins int,Bins int)\n    row format delimited fields terminated by ','\n    stored as textfile\n    location '/user/your_gaspar_name/work1/';\n</code></pre>\n<p>Now verify that your table was properly created.</p>\n<pre><code class=\"shell\">%jdbc(hive)\nselect * from your_gaspar_name.traffic_count limit 10;\n</code></pre>\n<p>Replace your_gaspar_name with your username and try the above commands in the next cells, create as many cells as needed.</p>\n<p>Note the first row. what did we do wrong?</p>\n"}]},"apps":[],"jobName":"paragraph_1521555185538_1739158077","id":"20180320-102737_103167059","dateCreated":"2018-03-20T15:13:05+0100","dateStarted":"2018-03-21T08:55:40+0100","dateFinished":"2018-03-21T08:55:40+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:19314"},{"text":"%jdbc(hive)","user":"ebouille","dateUpdated":"2018-03-21T04:24:09+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521597526899_-1482790124","id":"20180321-025846_71567959","dateCreated":"2018-03-21T02:58:46+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:19315"},{"text":"%md\n### Import one day of Twitter data into Hive\n\nCreate an external table  from HDFS dir __/datasets/twitter_one_day__ and call it __twitter_one_day__. The table should have a single column named __json__ of type __string__. Do not forget to use your own database (gaspar name)!\n\nA few hints:\n(1) You can explore the __/datasets/twitter_one_day__ directory from your terminal with the __hdfs dfs -ls__ command. You will notice that the files are in bzip2 format. Do not worry about that, Hive knows how to handle compressed text files automatically.\n(2) The files have only one field per line\n(3) If you do not specify the row format, the default format __fields terminated by '\\n'__ will be used.\n\nAfter the table __twitter_one_day__ is created, verify the content of its first row with a select command (limit 1). Use the output of the select query to identify the json fields where the the language and the timestamp information of the tweet are stored. You can use __http://jsonprettyprint.com/__ to pretty print the json string.\n\n","user":"ebouille","dateUpdated":"2018-03-21T08:54:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Import one day of Twitter data into Hive</h3>\n<p>Create an external table  from HDFS dir <strong>/datasets/twitter_one_day</strong> and call it <strong>twitter_one_day</strong>. The table should have a single column named <strong>json</strong> of type <strong>string</strong>. Do not forget to use your own database (gaspar name)!</p>\n<p>A few hints:\n<br  />(1) You can explore the <strong>/datasets/twitter_one_day</strong> directory from your terminal with the <strong>hdfs dfs -ls</strong> command. You will notice that the files are in bzip2 format. Do not worry about that, Hive knows how to handle compressed text files automatically.\n<br  />(2) The files have only one field per line\n<br  />(3) If you do not specify the row format, the default format <strong>fields terminated by '\\n'</strong> will be used.</p>\n<p>After the table <strong>twitter_one_day</strong> is created, verify the content of its first row with a select command (limit 1). Use the output of the select query to identify the json fields where the the language and the timestamp information of the tweet are stored. You can use <strong>http://jsonprettyprint.com/</strong> to pretty print the json string.</p>\n"}]},"apps":[],"jobName":"paragraph_1521555185539_1738773328","id":"20180320-133148_586015451","dateCreated":"2018-03-20T15:13:05+0100","dateStarted":"2018-03-21T08:54:58+0100","dateFinished":"2018-03-21T08:54:58+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19316"},{"text":"%jdbc(hive)","user":"ebouille","dateUpdated":"2018-03-21T09:33:23+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521597769923_2111501808","id":"20180321-030249_154474664","dateCreated":"2018-03-21T03:02:49+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:19317"},{"text":"%md\n### Twitter language frequency\n\nCompute the language frequencies, sorted in decreasing order of popularity. You should only use standard SQL group and count commands.\n\nTake advantage of the Zeppelin graph toolbar that appears with the results.\n\n```shell\n%jdbc(hive)\nwith q as (\n    select\n        get_json_object(json, '$.lang') as lang\n    from CHANGEME.twitter_one_day\n)\n\nselect lang, count(*) as count\nfrom q\ngroup by lang\norder by count desc;\n```\n\nYou can try different visualizations of the results with the embedded Zeppelin graph interface.\n\nFor further reading see the distinction between __group by__ and __sort by__ in __https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy__ . You can try this at home.\n","user":"ebouille","dateUpdated":"2018-03-21T10:09:48+0100","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{"0":{"graph":{"mode":"pieChart","height":300,"optionOpen":true,"setting":{"multiBarChart":{"stacked":false}},"commonSetting":{},"keys":[{"name":"lang","index":0,"aggr":"sum"}],"groups":[],"values":[{"name":"count","index":1,"aggr":"sum"}]},"helium":{}}},"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"editorHide":false,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Twitter language frequency</h3>\n<p>Compute the language frequencies, sorted in decreasing order of popularity. You should only use standard SQL group and count commands. Take advantage of the embedded Zeppelin graph interface in order to plot the results.</p>\n<pre><code class=\"shell\">%jdbc(hive)\nwith q as (\n    select\n        get_json_object(json, '$.lang') as lang\n    from CHANGEME.twitter_one_day\n)\n\nselect lang, count(*) as count\nfrom q\ngroup by lang\norder by count desc;\n</code></pre>\n<p>You can try different visualizations of the results with the embedded Zeppelin graph interface.</p>\n<p>For further reading see the distinction between <strong>group by</strong> and <strong>sort by</strong> in <strong>https://cwiki.apache.org/confluence/display/Hive/LanguageManual+SortBy</strong> . You can try this at home.</p>\n"}]},"apps":[],"jobName":"paragraph_1521555185543_1737234333","id":"20180320-135324_2117720219","dateCreated":"2018-03-20T15:13:05+0100","dateStarted":"2018-03-21T09:35:06+0100","dateFinished":"2018-03-21T09:35:06+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19318"},{"text":"%jdbc(hive)\nwith q as (\n    select\n        get_json_object(json, '$.lang') as lang\n    from CHANGEME.twitter_one_day\n)\n\nselect lang, count(*) as count\nfrom q\ngroup by lang\norder by count desc;","user":"ebouille","dateUpdated":"2018-03-21T09:33:09+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521602618863_766709691","id":"20180321-042338_1447379256","dateCreated":"2018-03-21T04:23:38+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:19319"},{"text":"%md\n### Extract the timeseries of languages used in twitter.\n\nIn this exercise, you will only keep the language and timestamp information (in ms since 01.01.1971 00:00:00 +00:00) of each tweet from the __twitter_one_day__ table. The result will be a table with a column __lang__ of type string and a column __time__ of type timestamp. You will store this result into a new table called __twitter_lang__.\n\nWe provide part of the query. You must fix all CHANGEME as needed in order to perform the above operation. Use the drop table command if you do not get the table right the first time.\n\n```shell\n%jdbc(hive)\n\ncreate table CHANGEME\nstored as parquet\nas\nselect\n    get_json_object(json, CHANGEME) as CHANGEME,\n    from_utc_timestamp(cast(CHANGEME as bigint), 'UTC') as CHANGEME\nfrom CHANGEME;\n```\n\n","user":"ebouille","dateUpdated":"2018-03-21T10:08:56+0100","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Extract the timeseries of languages used in twitter.</h3>\n<p>In this exercise, you will only keep the language and timestamp information of each tweet from the <strong>twitter_one_day</strong> table. The result will be a table with a column <strong>lang</strong> of type string and a column <strong>time</strong> of type timestamp. You will store this result into a new table called <strong>twitter_lang</strong>.</p>\n<p>We provide part of the query. You must fix all CHANGEME as needed in order to perform the above operation. Use the drop table command if you do not get the table right the first time.</p>\n<pre><code class=\"shell\">%jdbc(hive)\n\ncreate table CHANGEME\nstored as parquet\nas\nselect\n    get_json_object(json, '$.CHANGEME') as CHANGEME,\n    from_utc_timestamp(cast(CHANGEME as bigint), 'UTC') as CHANGEME\nfrom CHANGEME;\n</code></pre>\n"}]},"apps":[],"jobName":"paragraph_1521555185542_1737619082","id":"20180320-134103_2004356345","dateCreated":"2018-03-20T15:13:05+0100","dateStarted":"2018-03-21T04:44:52+0100","dateFinished":"2018-03-21T04:44:52+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19320"},{"text":"%jdbc(hive)\n\ncreate table CHANGEME\nstored as parquet\nas\nselect\n    get_json_object(json, '$.CHANGEME') as CHANGEME,\n    from_utc_timestamp(cast(CHANGEME as bigint), 'UTC') as CHANGEME\nfrom CHANGEME;","user":"ebouille","dateUpdated":"2018-03-21T09:32:32+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521602666923_-1654252982","id":"20180321-042426_1858484630","dateCreated":"2018-03-21T04:24:26+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:19321"},{"text":"%md\n### The mysterious Hive query\n\nCan you guess the meaning of this Hive command?\n\n```shell\n%jdbc(hive)\ncreate table CHANGEME.twitter_lang_csv\nrow format delimited\n    fields terminated by ','\n    escaped by '\"'\n    lines terminated by '\\n'\nstored as textfile\nas\nwith q as (\n    select \n        lang,\n        cast(date_format(time, 'HH') as int) as hour\n    from CHANGEME\n)\nselect lang, hour, count(*) as count\nfrom q\ngroup by lang, hour;\n```\n\nReplace all CHANGME occurences as appropriate and try it.","user":"ebouille","dateUpdated":"2018-03-21T09:36:15+0100","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>The mysterious Hive query</h3>\n<p>Can you guess the meaning of this Hive command?</p>\n<pre><code class=\"shell\">%jdbc(hive)\ncreate table CHANGEME.twitter_lang_csv\nrow format delimited\n    fields terminated by ','\n    escaped by '\"'\n    lines terminated by '\\n'\nstored as textfile\nas\nwith q as (\n    select \n        lang,\n        cast(date_format(time, 'HH') as int) as hour\n    from CHANGEME\n)\nselect lang, hour, count(*) as count\nfrom q\ngroup by lang, hour;\n</code></pre>\n<p>Replace all CHANGME occurences as appropriate and try it.</p>\n"}]},"apps":[],"jobName":"paragraph_1521555185543_1737234333","id":"20180320-135452_2072033476","dateCreated":"2018-03-20T15:13:05+0100","dateStarted":"2018-03-21T09:36:15+0100","dateFinished":"2018-03-21T09:36:15+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19322"},{"text":"%jdbc(hive)\ncreate table CHANGEME.twitter_lang_time\nstored as parquet\nas\nwith q as (\n    select \n        lang,\n        cast(date_format(time, 'HH') as int) as hour\n    from CHANGEME\n)\nselect lang, hour, count(*) as count\nfrom q\ngroup by lang, hour;","user":"ebouille","dateUpdated":"2018-03-21T09:46:07+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"sql"},"editorMode":"ace/mode/sql"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521621384868_2075729577","id":"20180321-093624_789131470","dateCreated":"2018-03-21T09:36:24+0100","dateStarted":"2018-03-21T09:37:22+0100","dateFinished":"2018-03-21T09:37:22+0100","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:19323"},{"text":"%md\n### Display language frequencies at different times of the day\n\nIn the next query you will display the frequecny tweets in Portuguese (pt) and Korean (ko) for each hour of the day.\n\nHints: (1) Only keep the tweets where the hour is not null and the lang is in ('pt', 'ko'), (2) use the SQL __where__ clause.\n\nSelect the bar chart view in Zeppelin graph toolbar that appears with the result. Then the open the settings and arrange the output fields (drag and drop) in the keys, groups and values properties until you get an insightful plot.\n\n\n","user":"ebouille","dateUpdated":"2018-03-21T10:14:43+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":true,"language":"markdown"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521621779930_-1897437686","id":"20180321-094259_347812219","dateCreated":"2018-03-21T09:42:59+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:19324","dateFinished":"2018-03-21T10:14:43+0100","dateStarted":"2018-03-21T10:14:43+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Display language frequencies at different times of the day</h3>\n<p>In the next query you will display the frequecny tweets in Portuguese (pt) and Korean (ko) for each hour of the day.</p>\n<p>Hints: (1) Only keep the tweets where the hour is not null and the lang is in ('pt', 'ko'), (2) use the SQL <strong>where</strong> clause.</p>\n<p>Select the bar chart view in Zeppelin graph toolbar that appears with the result. Then the open the settings and arrange the output fields (drag and drop) in the keys, groups and values properties until you get an insightful plot.</p>\n"}]}},{"text":"%jdbc(hive)\n","dateUpdated":"2018-03-21T10:20:19+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"sql"},"colWidth":12,"editorMode":"ace/mode/sql","editorHide":false,"results":{"0":{"graph":{"mode":"multiBarChart","height":300,"optionOpen":false,"setting":{"multiBarChart":{"stacked":false},"pieChart":{},"stackedAreaChart":{"style":"stream"}},"commonSetting":{},"keys":[{"name":"twitter_lang_csv.hour","index":1,"aggr":"sum"}],"groups":[{"name":"twitter_lang_csv.lang","index":0,"aggr":"sum"}],"values":[{"name":"twitter_lang_csv.count","index":2,"aggr":"sum"}]},"helium":{}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521555185544_1735310588","id":"20180320-135453_712665411","dateCreated":"2018-03-20T15:13:05+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:19325"},{"user":"ebouille","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":true,"language":"markdown"},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521624060771_875679668","id":"20180321-102100_1524832575","dateCreated":"2018-03-21T10:21:00+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:20244","text":"%md\n\n### Find the most retweeted users\n\nIn the next queries you will compute the list of the 50 most retweeted users.\n\nYou will first create the table __twitter_users__ and store it in __parquet__ format.\n\n```shell\n%jdbc(hive)\ncreate table CHANGEME\nstored as parquet\nas\nselect CHANGEME(json, '$.retweeted_status.user.screen_name') as name\nfrom CHANGEME\nwhere CHANGEME(json, '$.retweeted_status') is not null;\n```\n\nNext you will create the table of  the 50 most popular users in decreasing order of retweets for the day.\n\n```shell\n%jdbc(hive)\ncreate table CHANGEME.twitter_users_count\nrow format delimited\n    fields terminated by ','\n    escaped by '\"'\n    lines terminated by '\\n'\nstored as textfile\nselect name, count(*) as count\nfrom CHANGEME.twitter_users\ngroup by name\norder by count desc\nlimit 50;\n```\n\nNote that we have created the table in textfile format, and we have specified a row format as a CSV file. To wrap this exercise up, try to find this file in HDFS and use the __hdfs dfs -cat__ command to visualize it.\n","dateUpdated":"2018-03-21T10:35:11+0100","dateFinished":"2018-03-21T10:35:11+0100","dateStarted":"2018-03-21T10:35:11+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h3>Find the most retweeted users</h3>\n<p>In the next queries you will compute the list of the 50 most retweeted users.</p>\n<p>You will first create the table <strong>twitter_users</strong> and store it in <strong>parquet</strong> format.</p>\n<pre><code class=\"shell\">%jdbc(hive)\ncreate table CHANGEME\nstored as parquet\nas\nselect CHANGEME(json, '$.retweeted_status.user.screen_name') as name\nfrom CHANGEME\nwhere CHANGEME(json, '$.retweeted_status') is not null;\n</code></pre>\n<p>Next you will create the table of  the 50 most popular users in decreasing order of retweets for the day.</p>\n<pre><code class=\"shell\">%jdbc(hive)\ncreate table CHANGEME.twitter_users_count\nrow format delimited\n    fields terminated by ','\n    escaped by '\"'\n    lines terminated by '\\n'\nstored as textfile\nselect name, count(*) as count\nfrom CHANGEME.twitter_users\ngroup by name\norder by count desc\nlimit 50;\n</code></pre>\n<p>Note that we have created the table in textfile format, and we have specified a row format as a CSV file. To wrap this exercise up, try to find this file in HDFS and use the <strong>hdfs dfs -cat</strong> command to visualize it.</p>\n"}]}},{"dateUpdated":"2018-03-20T15:13:05+0100","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521555185548_1733771593","id":"20180320-111048_1278675940","dateCreated":"2018-03-20T15:13:05+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:19328"},{"dateUpdated":"2018-03-20T15:13:05+0100","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521555185550_1734541090","id":"20180320-111112_142668305","dateCreated":"2018-03-20T15:13:05+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:19329"},{"text":"%md\n","dateUpdated":"2018-03-21T04:30:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1521555185551_1734156341","id":"20180320-143329_5934928","dateCreated":"2018-03-20T15:13:05+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:19330"}],"name":"DSLAB-Week5-1","id":"2DBVM6KTX","angularObjects":{"2CHS8UYQQ:shared_process":[],"2CK8A9MEG:shared_process":[],"2CKAY1A8Y:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}